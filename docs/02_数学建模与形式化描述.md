# Phase 1: Mathematical Modeling & Problem Formulation

> **Project:** Intelligent Cross-Domain Scheduling for Large Model Training (Task 3)
> **Target:** CCF-A Class Conference (System/AI Track)
> **Version:** v1.0
> **Date:** 2026-01-26

**核心假设**: 本研究假设**同构算力环境**（各域 GPU 型号相同），核心挑战聚焦于**跨域网络动态性**和**通信优化**。

---

## 1. 系统模型 (System Model)

我们将"东数西算"背景下的跨域算力网络建模为一个**动态加权有向图** (Dynamic Weighted Directed Graph)。

### 1.1 同构算力节点 (Homogeneous Compute Nodes)

集合 $\mathcal{V} = \{v_1, v_2, ..., v_K\}$ 表示分布在不同地理位置的 $K$ 个算力域（Data Centers）。

每个节点 $v_k$ 包含**同构**计算资源，定义为元组：
$$v_k = (C, M, \mathcal{D}_k)$$

* $C$: 标准化浮点计算能力 (Normalized FLOPS)，**各域相同**
* $M$: 显存总容量 (Total Memory Capacity)，**各域相同**
* $\mathcal{D}_k$: 域内设备的集合 (Device Topology)，例如 $\mathcal{D}_k = \{GPU_1, GPU_2, ..., GPU_{N_k}\}$

**同构假设的意义**：
- 简化计算模型，各域计算时间可直接比较
- 核心差异在于**网络连接状态**，而非算力差异
- 聚焦于网络动态性对调度决策的影响

### 1.2 动态网络链路 (Dynamic Network Edges) - 核心创新点

集合 $\mathcal{E}$ 表示域间广域网连接。
每条边 $e_{ij} \in \mathcal{E}$ 在时刻 $t$ 的状态定义为：
$$e_{ij}(t) = (B_{ij}(t), L_{ij}(t), \gamma_{ij})$$

* $B_{ij}(t)$: **实时可用带宽** (Available Bandwidth)，核心动态变量
* $L_{ij}(t)$: **传输延迟** (Latency)，随网络拥塞变化
* $\gamma_{ij}$: 单位流量的通信成本系数（用于计算 billing cost）

**网络动态性模型**：
$$B_{ij}(t) = B_{ij}^{base} \cdot (1 + \epsilon_{ij}(t))$$

其中 $\epsilon_{ij}(t) \in [-0.3, 0.3]$ 是时变的网络波动因子，服从某种随机过程（如 Ornstein-Uhlenbeck 过程）。

**关键约束**：
- 域间带宽：$B_{inter} < 10$ Gbps（通常 1-5 Gbps）
- 域内带宽：$B_{intra} \approx 100-400$ Gbps（NVLink/PCIe）
- 域间延迟：$L_{inter} > 20$ ms（通常 20-100 ms）
- 域内延迟：$L_{intra} < 1$ ms

---

## 2. 大模型训练任务形式化 (Task Formulation)

大模型训练任务 $\mathcal{T}$ 被建模为一个**计算图** (Computation Graph)。我们将模型划分为 $N$ 个逻辑层 (Layers) 序列 $\{l_1, l_2, ..., l_N\}$。

### 2.1 层特征 (Layer Characteristics)

每层 $l_i$ 具有以下特征：
$$l_i = (F_i, M_i^{act}, M_i^{param}, D_i^{in}, D_i^{out})$$

* $F_i$: 计算量 (FLOPs)
* $M_i^{act}$: 激活值内存占用
* $M_i^{param}$: 参数内存占用
* $D_i^{in}, D_i^{out}$: 输入/输出数据量

### 2.2 并行策略空间 (Parallelism Space)

对于每一层 $l_i$，调度器需要决定其并行策略 $P_i$：
$$P_i = (pp_i, dp_i, tp_i)$$

* $pp_i \in \{1, 2, ..., K\}$: 流水线并行 (Pipeline Parallelism)，决定该层被分配到哪个**域**
* $dp_i \in \mathbb{Z}^+$: 数据并行度 (Data Parallelism degree)
* $tp_i \in \mathbb{Z}^+$: 张量并行度 (Tensor Parallelism degree)

**约束 (Constraint):** 若 $pp_i \neq pp_{i+1}$，则产生**跨域通信开销**。

---

## 3. 多目标优化问题 (Multi-Objective Optimization)

根据课题要求，我们需要构建一个联合优化函数。这是论文的核心数学贡献。

### 3.1 目标函数分解

我们要寻找最优策略 $\pi^*$ 以最大化累积奖励 $J(\pi)$：
$$J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{T} \gamma^t R_t \right]$$

单步奖励 $R_t$ 由三个加权分量组成：
$$R_t = w_1 \cdot R_t^{eff} + w_2 \cdot R_t^{util} - w_3 \cdot R_t^{cost}$$

#### (1) 训练效率 ($R^{eff}$)

目标是最小化单次迭代时间 $T_{step}$。在流水线并行下，$T_{step}$ 由最慢的 Stage 和气泡时间 (Bubble Time) 决定：

$$T_{step} = T_{compute} + T_{comm} + T_{bubble}$$

其中：
- $T_{compute} = \max_{k \in K} \{ T_k^{compute} \}$（最慢域的计算时间）
- $T_{comm} = \sum_{(i,j): pp_i \neq pp_j} \frac{D_i^{out}}{B_{pp_i, pp_j}(t)}$（跨域通信时间）
- $T_{bubble}$：流水线气泡时间

我们将奖励定义为时间缩短的收益：
$$R_t^{eff} = \frac{T_{step}^{baseline} - T_{step}^{current}}{T_{step}^{baseline}}$$

*注：$T_{bubble}$ 的计算采用 GPipe 或 1F1B 调度的标准公式，如 $T_{bubble} = (K-1) \cdot T_{micro}$，其中 $K$ 是 Stage 数量。*

#### (2) 算力利用率 ($R^{util}$)

定义为所有活跃节点的平均利用率：
$$R_t^{util} = \frac{1}{K} \sum_{k=1}^{K} U_k(t)$$

其中 $U_k(t) = \frac{T_k^{compute}}{T_{step}}$ 是域 $k$ 的 GPU 利用率。

目标是提升该值 $\geq 35\%$。

#### (3) 通信成本 ($R^{cost}$)

仅计算跨域流量产生的费用：
$$R_t^{cost} = \sum_{(i,j) \in \mathcal{E}_{cross}} \gamma_{ij} \cdot D_{ij}(t)$$

其中 $\mathcal{E}_{cross}$ 是跨域边集合，$D_{ij}(t)$ 是时刻 $t$ 的跨域数据传输量。

目标是降低该值 $\geq 30\%$。

### 3.2 权重自适应机制

权重 $w_1, w_2, w_3$ 可以采用以下策略：
1. **固定权重**: 根据业务优先级预设
2. **动态调整**: 基于当前网络状态自适应
3. **Pareto 优化**: 使用多目标 RL 方法（如 MORL）

---

## 4. Hi-PPO 分层 MDP 建模 (Hierarchical MDP Formulation)

为了满足 **< 1 秒决策延迟** 和 **分层调度机制** 的要求，我们将问题分解为双层 MDP。

### 4.1 Level 1: 域间调度器 (Inter-Domain Agent)

**解决问题**: Pipeline Stage 的划分与放置 (Coarse-grained)。

**Time Step**: 每个 Epoch 或**网络状态显著变化时**触发。

**State Space ($\mathcal{S}^{high}$)**:
$$s^{high} = (h_{topo}, \vec{L}, B(t), L(t))$$

* $h_{topo} \in \mathbb{R}^d$: 基于 GNN 的全网拓扑嵌入向量
* $\vec{L} \in \mathbb{R}^K$: 各域当前的负载/内存占用向量
* $B(t) \in \mathbb{R}^{K \times K}$: **域间带宽矩阵**（核心动态输入）
* $L(t) \in \mathbb{R}^{K \times K}$: **域间延迟矩阵**

**Action Space ($\mathcal{A}^{high}$)**:
$$a^{high} = \{c_1, c_2, ..., c_{K-1}\}$$

决定 $N$ 层模型在 $K$ 个域间的切分点 (Cut Points)。例如 $\{12, 24\}$ 表示：
- 层 1-12 在域 A
- 层 13-24 在域 B
- 层 25-N 在域 C

**Reward ($R^{high}$)**:
$$R^{high} = -\alpha \cdot T_{step} - \beta \cdot C_{comm}^{cross}$$

全局 $T_{step}$ 的负值 + 全局跨域通信成本惩罚。

### 4.2 Level 2: 域内调度器 (Intra-Domain Agent)

**解决问题**: 具体 GPU 映射与 DP/TP 策略 (Fine-grained)。

**Time Step**: 实时 (Real-time)，高频更新。

**State Space ($\mathcal{S}^{low}_k$)**:
$$s^{low}_k = (\vec{M}_k, G_k, Q_k)$$

* $\vec{M}_k \in \mathbb{R}^{N_k}$: 域内每张卡的显存剩余量
* $G_k$: 域内 NVLink/PCIe 拓扑状态
* $Q_k$: 分配给该域的子任务队列（来自上层决策）

**Action Space ($\mathcal{A}^{low}_k$)**:
$$a^{low}_k = (dp_k, tp_k, placement_k)$$

* $dp_k$: 数据并行度
* $tp_k$: 张量并行度
* $placement_k$: 具体的 Device Placement 决策

**Reward ($R^{low}_k$)**:
$$R^{low}_k = -\lambda_1 \cdot Frag_k - \lambda_2 \cdot T_{comm}^{intra}$$

最小化域内碎片 (Fragmentation) 和域内通信延迟。

### 4.3 层间交互机制 (Inter-Level Interaction)

**上层 → 下层**:
- 上层的 Action（切分点）决定了下层的任务分配
- 上层输出作为下层的**约束条件**

**下层 → 上层**:
- 下层的执行结果（实际 $T_{step}$）作为上层的 Reward 信号
- 下层的资源状态反馈给上层用于下一次决策

**协调机制**:
$$s^{high}_{t+1} = f(s^{high}_t, a^{high}_t, \{R^{low}_k\}_{k=1}^K)$$

---

## 5. 约束条件的数学表达 (Constraints Implementation)

在 PPO 训练中，我们需要通过 **Action Masking** 或 **Penalty** 来强制执行以下约束：

### 5.1 显存墙约束 (Memory Wall Constraint)

$$\sum_{l \in Stage_k} (M_l^{act} + M_l^{param}) \leq M_k^{available}, \quad \forall k$$

**实现方式**: 在 RL 的 Action layer 加入 Mask，禁止会导致 OOM (Out of Memory) 的动作。

### 5.2 网络感知约束 (Network-Awareness Constraint)

$$\text{If } B_{ij}(t) < B_{threshold}, \text{ then } penalty_{ij} = M \cdot D_{ij}$$

**实现方式**: 在 Reward 中加入巨大的惩罚项 (Big-M penalty)，如果将高通信量的两层切分到带宽受限的跨域链路上。

### 5.3 负载均衡约束 (Load Balancing Constraint)

$$\frac{\max_k T_k^{compute} - \min_k T_k^{compute}}{\bar{T}^{compute}} \leq \delta$$

**实现方式**: 在 Reward 中加入负载方差惩罚项。

### 5.4 分层协同响应约束 (Coordination Latency Constraint)

$$T_{coord} \leq 2s$$

**实现方式**: 记录上层决策到下层执行的端到端耗时，超限则加入惩罚项或触发降级策略。

### 5.5 负载波动稳定性约束 (Stability Under Load Fluctuation)

在负载波动 $\pm 20\%$ 的场景下，训练进度稳定性 $\ge 99\%$。

**实现方式**: 在仿真中注入负载扰动，统计训练中断率并加入惩罚项。

---

## 6. PPO 算法增强 (PPO Algorithm Enhancement)

### 6.1 基础 PPO 目标函数

$$L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]$$

其中 $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$，$\hat{A}_t$ 是优势函数估计。

### 6.2 针对跨域调度的增强

1. **时序信用分配**: 引入 GAE (Generalized Advantage Estimation) 处理延迟奖励
   $$\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$

2. **约束处理**: 使用 Lagrangian 方法处理带宽和内存约束
   $$L(\theta, \mu) = L^{CLIP}(\theta) - \mu \cdot g(s, a)$$

3. **网络状态编码**: 使用 GNN 编码动态网络拓扑
   $$h_{topo} = \text{GNN}(V, E, B(t), L(t))$$

4. **探索策略**: 采用 entropy bonus 鼓励探索
   $$L_{total} = L^{CLIP} + c_1 \cdot L_{VF} + c_2 \cdot H(\pi)$$

---

## 6.3 GNN 拓扑编码实现细节（补充）

**图定义**:
- 节点: 域 $v_k$
- 边: 跨域链路 $e_{ij}$
- 节点特征: $[U_k^{GPU}, U_k^{Mem}, Q_k]$
- 边特征: $[B_{ij}(t), L_{ij}(t), \gamma_{ij}]$

**编码**:
$$h_{topo} = \text{GNN}(\mathcal{V}, \mathcal{E}, X_v, X_e)$$

**建议结构**: 2 层 GraphSAGE/GAT + 全局池化（mean/max）。

---

## 6.4 上下层协调伪代码（补充）

```text
Input: s_high, {s_low_k}
1: h_topo <- GNN(B(t), L(t), R_k)
2: a_high <- pi_high(s_high, h_topo)
3: for each domain k:
4:     a_low_k <- pi_low_k(s_low_k | a_high)
5: Execute a = {a_high, a_low_k}
6: Compute R from T_step, U_k, C_comm
7: Update PPO for high/low policies
```

---

## 6.5 Action Masking 与约束处理（补充）

- **OOM 约束**: 若切分导致显存溢出，动作直接 Mask
- **带宽约束**: 当 $B_{ij}(t)$ 低于阈值时，对跨域切分施加惩罚或禁用
- **负载均衡**: 对过度倾斜的域内分配加入惩罚项

---

## 6.6 网络状态预测模型（补充）

**滑动窗口预测**:
$$\hat{B}_{ij}(t+\Delta t) = f_{predict}(B_{ij}(t-w:t))$$

**实现建议**:
- 轻量 GRU / Kalman 滤波
- 作为上层状态输入的附加特征

---

## 7. 符号表 (Notation Summary)

| 符号 | 含义 |
|------|------|
| $K$ | 计算域数量 |
| $N$ | 模型层数 |
| $N_k$ | 域 $k$ 的 GPU 数量 |
| $C$ | 标准化计算能力（同构） |
| $M$ | 显存容量（同构） |
| $B_{ij}(t)$ | 域 $i$ 到域 $j$ 的实时带宽 |
| $L_{ij}(t)$ | 域 $i$ 到域 $j$ 的实时延迟 |
| $\epsilon_{ij}(t)$ | 网络波动因子 |
| $T_{step}$ | 全局步长耗时 |
| $U_k$ | 域 $k$ 的利用率 |
| $s^{high}$ | 上层状态空间 |
| $s^{low}$ | 下层状态空间 |
| $a^{high}$ | 上层动作空间 |
| $a^{low}$ | 下层动作空间 |
| $R^{high}$ | 上层奖励 |
| $R^{low}$ | 下层奖励 |
| $w_1, w_2, w_3$ | 多目标权重系数 |

---

## 8. 与 SOTA 的差异化分析

| 方面 | MAST (OSDI'24) | Sailor (SOSP'25) | **Ours (Hi-PPO)** |
|------|----------------|------------------|-------------------|
| 调度粒度 | 任务级 | 任务级 | **算子/层级** |
| 网络感知 | 静态带宽 | 部分动态 | **全动态 + 预测** |
| 算法框架 | 启发式 | 单层 RL | **分层 RL (Hi-PPO)** |
| 决策延迟 | ~秒级 | ~秒级 | **<1秒** |
| 多目标优化 | 单目标 | 双目标 | **三目标 Pareto** |

---

*本文档将作为算法实现的蓝图和论文 Methodology 章节的基础*
*核心假设：同构算力环境，聚焦网络动态性优化*
